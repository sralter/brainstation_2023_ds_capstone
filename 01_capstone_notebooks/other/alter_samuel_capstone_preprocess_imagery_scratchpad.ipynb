{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3639467-b266-47fd-90f3-d4cef4d82b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'usgs.api' has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m landsat8_scene_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLC80290462015135LGN00\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Submit requests to USGS servers\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m(hyperion_dataset, node, [hyperion_scene_id])\n\u001b[1;32m     16\u001b[0m api\u001b[38;5;241m.\u001b[39mmetadata(landsat8_dataset, node, [landsat8_scene_id])\n\u001b[1;32m     18\u001b[0m usgs\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mmetadata(dataset, node, entityids, extended\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'usgs.api' has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "from usgs import api\n",
    "\n",
    "# Set the EarthExplorer catalog\n",
    "node = 'EE'# this indicates earth explorer website\n",
    "\n",
    "# Set the Hyperion and Landsat 8 dataset\n",
    "hyperion_dataset = 'EO1_HYP_PUB'\n",
    "landsat8_dataset = 'LANDSAT_8'\n",
    "\n",
    "# Set the scene ids\n",
    "hyperion_scene_id = 'EO1H1820422014302110K2_SG1_01'\n",
    "landsat8_scene_id = 'LC80290462015135LGN00'\n",
    "\n",
    "# Submit requests to USGS servers\n",
    "api.metadata(hyperion_dataset, node, [hyperion_scene_id])\n",
    "api.metadata(landsat8_dataset, node, [landsat8_scene_id])\n",
    "\n",
    "usgs.api.metadata(dataset, node, entityids, extended=False, api_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf7b9d-89e7-4051-a6c0-70d2a759ffd2",
   "metadata": {},
   "source": [
    "## Sentinelsat and Copernicus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0f3082-5d03-4d20-b1bc-96060d1e1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/satellite-imagery-access-and-analysis-in-python-jupyter-notebooks-387971ece84b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14e44bb-0a7b-421e-940d-ea63f728999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sentinelsat.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4027c8-373f-4553-927a-58c0e9fd2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI\n",
    "\n",
    "user = 'COPERNICUS_USERNAME' \n",
    "password = 'COPERNICUS_PASSWORD' \n",
    "api = SentinelAPI(user, password, 'https://scihub.copernicus.eu/dhus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea715e2-4567-47f5-ab08-2693f76a9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium \n",
    "\n",
    "# santa monica bounds file named nReserve\n",
    "nReserve = gpd.read_file('/Users/sra/Files/brainstation_2023_ds_capstone/brainstation_2023_ds_capstone/01_capstone_data/shapefiles/santa_monica_bounds/sm_bounds.geojson')\n",
    "\n",
    "# empty base map in Folium centered around the bounding box\n",
    "m = folium.Map([34.08483,-118.70617], zoom_start=12)\n",
    "folium.GeoJson(nReserve).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78269d-291d-4ceb-b9ac-508536da6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "footprint = None\n",
    "for i in nReserve['geometry']:\n",
    "    footprint = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c70cb-4ee8-49f2-88e8-6ae18697f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = api.query(footprint,\n",
    "                     #date = ('20190601', '20190626'),\n",
    "                     date = ('20220101', '20221231'),\n",
    "                     platformname = 'Sentinel-2',\n",
    "                     processinglevel = 'Level-2A',\n",
    "                     cloudcoverpercentage = (0,15)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb912512-3f28-4d65-94cf-99cb03e7c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208963e6-0e79-46af-9f66-590eff123b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_gdf = api.to_geodataframe(products)\n",
    "products_gdf_sorted = products_gdf.sort_values(['cloudcoverpercentage'], ascending=[True])\n",
    "products_gdf_sorted\n",
    "products_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f230ed-4ad9-4217-b15b-6b7daa9923e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.download('becdf74b-fb47-4010-84f1-2c271a501266')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d739d0-57cb-48cd-a32b-a52ee8164484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "R10 = '/Users/sra/Files/brainstation_2023_ds_capstone/brainstation_2023_ds_capstone/01_capstone_data/notebooks/S2A_MSIL2A_20221224T183801_N0509_R027_T11SLT_20221224T213852.SAFE/GRANULE/L2A_T11SLT_A039206_20221224T184116/IMG_DATA/R10m'\n",
    "b4 = rio.open(R10+'/T11SLT_20221224T183801_B04_10m.jp2')\n",
    "b3 = rio.open(R10+'/T11SLT_20221224T183801_B03_10m.jp2')\n",
    "b2 = rio.open(R10+'/T11SLT_20221224T183801_B02_10m.jp2')\n",
    "\n",
    "\n",
    "# Create an RGB image \n",
    "with rio.open('RGB.tiff','w',driver='Gtiff', width=b4.width, height=b4.height, \n",
    "              count=3,crs=b4.crs,transform=b4.transform, dtype=b4.dtypes[0]) as rgb:\n",
    "    rgb.write(b2.read(1),1) \n",
    "    rgb.write(b3.read(1),2) \n",
    "    rgb.write(b4.read(1),3) \n",
    "    rgb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ee1df-8185-49f3-ae09-b07b4eb0f255",
   "metadata": {},
   "source": [
    "## another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75705c98-d298-421e-9851-cc82ae662780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filenames\n",
    "\n",
    "# raster in\n",
    "rasin='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_merge_clip.tif'\n",
    "# CRS:\n",
    "# Coordinate Reference System (CRS)\n",
    "# Name\t\t\t\tEPSG:26911 - NAD83 / UTM zone 11N\n",
    "# Units\t\t\t\tmeters\n",
    "# Method\t\t\tUniversal Transverse Mercator (UTM)\n",
    "# Celestial body\tEarth\n",
    "# Reference\t\t\tStatic (relies on a datum which is plate-fixed)\n",
    "\n",
    "\n",
    "# shapefile in\n",
    "shpin='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/perimeters_sm/santa_monica_fire_perimeters_le2018_merge_shp.shp'\n",
    "# I reprojected the shapefile to the same crs as the raster above in QGIS\n",
    "# Coordinate Reference System (CRS)\n",
    "\n",
    "# Name\n",
    "# EPSG:26911 - NAD83 / UTM zone 11N\n",
    "# Units\n",
    "# meters\n",
    "# Method\n",
    "# Universal Transverse Mercator (UTM)\n",
    "# Celestial body\n",
    "# Earth\n",
    "# Reference\n",
    "# Static (relies on a datum which is plate-fixed)\n",
    "\n",
    "\n",
    "# raster out\n",
    "rasout='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_sm_fire.tif'\n",
    "\n",
    "#                 out     in\n",
    "result=gdal.Warp(rasout,rasin,cutlineDSName=shpin)\n",
    "\n",
    "iface.addRasterLayer(rasout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13272a6-a454-443d-b5e6-a3b16e07d447",
   "metadata": {},
   "source": [
    "## https://opensourceoptions.com/blog/how-to-clip-a-raster-to-a-polygon-or-an-extent-with-python-extract-by-mask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e96dc4-5e70-4320-a888-34cd87b45d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# fn_in = r\"/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_merge_clip.tif\"\n",
    "rast_in='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/m_3411849_se_11_060_20180722.tif'\n",
    "\n",
    "# fn_clip = '/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_merge_sm_fire.tif'\n",
    "\n",
    "\n",
    "# fn_poly = r\"path/to/polygon.gpkg\"\n",
    "\n",
    "# # gdal.Warp(fn_clip, fn_in, cutlineDSName=fn_poly, cutlineLayer='polygon', cropToCutline=True)\n",
    "\n",
    "# # from osgeo import gdal\n",
    "# # fn_in = r\"..\\..\\course-gdal-python\\data\\input\\USGS_one_meter_x64y486_ID_FEMAHQ_2018.tif\"\n",
    "# fn_in=r'/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_merge_clip.tif'\n",
    "\n",
    "# # fn_poly = r\"..\\..\\course-gdal-python\\data\\input\\polygon.gpkg\"\n",
    "# fn_poly=r'/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/perimeters_sm/santa_monica_fire_perimeters_le2018_merge.geojson'\n",
    "\n",
    "# # fn_clip = '../data/output/clip_polygon.tif'\n",
    "# fn_clip='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_merge_clip_fire.tif'\n",
    "\n",
    "\n",
    "# ortho_fire=gdal.Warp(fn_clip, fn_in, cutlineDSName=fn_poly, cutlineLayer='polygon', cropToCutline=True)\n",
    "# ortho_fire\n",
    "# # gdal.Warp(fn_clip2, fn_in, cutlineDSName=fn_poly, cutlineLayer='polygon', cropToCutline=False)\n",
    "# # where = \"ID = '1'\"\n",
    "# # gdal.Warp(fn_clip_where1, fn_in, cutlineDSName=fn_poly, cutlineLayer='polygon', cropToCutline=True, cutlineWhere=where)\n",
    "# # where = \"ID = '2'\"\n",
    "# # gdal.Warp(fn_clip_where2, fn_in, cutlineDSName=fn_poly, cutlineLayer='polygon', cropToCutline=True, cutlineWhere=where)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75270f-bc63-4db3-9d47-475250b01941",
   "metadata": {},
   "source": [
    "## rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967f2b6-a975-4a79-9de0-0dfccb37ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "\n",
    "show((data,4),cmap='terrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a220a0-9848-40d8-9567-1cca4c22b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open raster in read mode\n",
    "data=rasterio.open(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ad9ae0-aafa-428f-8eef-92a6fb3c49d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mdata\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d67c6-152e-47c2-a305-82fb57053ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to insert geometry into a geodataframe\n",
    "\n",
    "polygon=r'/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/santa_monica_water_bounds/sm_bounds_water_epsg26911_nad83_utm11n.geojson'\n",
    "\n",
    "geo=gpd.GeoDataFrame({'geometry':polygon},index=[0],crs=from_epsg(4326))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289f71f-93c6-495d-87c1-797f5d905404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to re-project into the same coordinate system as the raster data\n",
    "\n",
    "geo = geo.to_crs(crs=data.crs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cfb8d-6ec5-4f11-b72c-df1854022dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates of the geometry in a format that rasterio wants them\n",
    "def getFeatures(gdf):\n",
    "    \"\"\"Function to parse features from GeoDataFrame in such a manner that rasterio wants them\"\"\"\n",
    "    import json\n",
    "    return [json.loads(gdf.to_json())['features'][0]['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04923757-d761-4664-98a6-fbd74a8eb801",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coords \u001b[38;5;241m=\u001b[39m \u001b[43mgetFeatures\u001b[49m(geo)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(coords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "coords = getFeatures(geo)\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9839a4a2-c53b-40e9-a669-477774876b4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we are ready to clip the raster with the polygon \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# using the coords variable that we just created. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Clipping the raster can be done easily with the mask function \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# that we imported in the beginning from rasterio, and specifying clip=True.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m out_img, out_transform \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m(raster\u001b[38;5;241m=\u001b[39mdata, shapes\u001b[38;5;241m=\u001b[39mcoords, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we are ready to clip the raster with the polygon \n",
    "# using the coords variable that we just created. \n",
    "# Clipping the raster can be done easily with the mask function \n",
    "# that we imported in the beginning from rasterio, and specifying clip=True.\n",
    "\n",
    "out_img, out_transform = mask(raster=data, shapes=coords, crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216dd056-9ea0-49c4-a04e-01eb46bf316b",
   "metadata": {},
   "source": [
    "## clip raster to polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18877818-9218-418e-91b8-0a5a77d6180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a9d4e-959e-42f6-9ccd-bef059143ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapely.geos.geos_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6970403-0304-43ca-8a59-2c5fd845e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, merge perimeters\n",
    "# https://stackoverflow.com/questions/61035170/merging-polygon-shapefiles-in-python\n",
    "\n",
    "# this was done in QGIS for now. View merged perimeters, \n",
    "# to confirm only one polygon in the dataset\n",
    "sm_perim_merge=gpd.read_file('/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/perimeters_sm/santa_monica_fire_perimeters_le2018_merge.geojson')\n",
    "sm_perim_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bc7b7-5bbc-49b0-acb7-e771d7670694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates from geometry of sm_perim_merge\n",
    "\n",
    "sm_perim_merge['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd15f9-19ce-4e87-bad9-1db79d675d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "\n",
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241af23-9f3b-4c06-ae06-f58f864e9d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f908e-02ef-4e8c-9642-0d58186f3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# filepaths\n",
    "target_tiff_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/img_patch_fire2.tif'\n",
    "output_location_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches'\n",
    "\n",
    "# load the image using Pillow\n",
    "with Image.open('/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/img_patch_fire2.tif') as img:\n",
    "    # convert the image to a format that OpenCV can handle (e.g., JPEG)\n",
    "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# display the image using OpenCV\n",
    "cv2.imshow('image', img_cv)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a8253-005c-456d-b2b1-e030128200c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing the images\n",
    "directory = \"/Users/sra/temp\"\n",
    "\n",
    "# Set the desired size of the square image\n",
    "desired_size = 128\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Get the size of the image\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Check if the image is already square\n",
    "        if width == height:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the new size of the image\n",
    "        if width < height:\n",
    "            new_width = desired_size\n",
    "            new_height = int(desired_size * height / width)\n",
    "        else:\n",
    "            new_height = desired_size\n",
    "            new_width = int(desired_size * width / height)\n",
    "        \n",
    "        # Resize the image\n",
    "        new_image = image.resize((new_width, new_height))\n",
    "        \n",
    "        # Create a new blank square image\n",
    "        new_square_image = Image.new(\"RGB\", (desired_size, desired_size), (255, 255, 255))\n",
    "        \n",
    "        # Paste the resized image onto the center of the square image\n",
    "        left = (desired_size - new_width) // 2\n",
    "        top = (desired_size - new_height) // 2\n",
    "        new_square_image.paste(new_image, (left, top))\n",
    "        \n",
    "        # Save the new square image\n",
    "        new_image_path = os.path.join(directory, f\"{os.path.splitext(filename)[0]}_square.jpg\")\n",
    "        new_square_image.save(new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70771e33-bdb2-48fd-803d-12fc2db5e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "source_folder = '/Users/sra/temp'\n",
    "destination_folder = '/Users/sra/temp2'\n",
    "\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        img_path = os.path.join(source_folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w, _ = img.shape\n",
    "        if h != w:\n",
    "            # stretch the image to make it square\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "        cv2.imwrite(os.path.join(destination_folder, filename), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa9c86-ffb7-4899-ab2a-f07c89a48217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define input and output directories\n",
    "# input_dir = \"input_folder/\"\n",
    "# output_dir = \"output_folder/\"\n",
    "\n",
    "# # loop through all files in input directory\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     # check if the file is a TIFF image\n",
    "#     if filename.endswith(\".tif\"):\n",
    "#         # open the image using PIL\n",
    "#         with Image.open(input_dir + filename) as img:\n",
    "#             width, height = img.size\n",
    "#             # check if the image is already square\n",
    "#             if width == height:\n",
    "#                 img.save(output_dir + filename)\n",
    "#             else:\n",
    "#                 # stretch the image to be square (128x128 pixels)\n",
    "#                 if width > height:\n",
    "#                     new_height = 128\n",
    "#                     new_width = int(width * new_height / height)\n",
    "#                     img = img.resize((new_width, new_height))\n",
    "#                     left = (new_width - 128) / 2\n",
    "#                     top = 0\n",
    "#                     right = (new_width + 128) / 2\n",
    "#                     bottom = 128\n",
    "#                     img = img.crop((left, top, right, bottom))\n",
    "#                     img.save(output_dir + filename)\n",
    "#                 else:\n",
    "#                     new_width = 128\n",
    "#                     new_height = int(height * new_width / width)\n",
    "#                     img = img.resize((new_width, new_height))\n",
    "#                     left = 0\n",
    "#                     top = (new_height - 128) / 2\n",
    "#                     right = 128\n",
    "#                     bottom = (new_height + 128) / 2\n",
    "#                     img = img.crop((left, top, right, bottom))\n",
    "#                     img.save(output_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b749c-734d-4972-af8c-2ac286580596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateNonSquareImages(input_path, output_path):\n",
    "    '''\n",
    "    \n",
    "    Separates non-square images from square images from a directory\n",
    "    and sends them to a separate user-defined folder.\n",
    "    \n",
    "    Square images are those whose width and height are the same.\n",
    "    For example, a 128x128-pixel image is square. 29x128-pixel\n",
    "    image is not square.\n",
    "    \n",
    "    ----\n",
    "    Inputs\n",
    "    \n",
    "    >input_path\n",
    "    directory location of where the images (both square and non-)\n",
    "    are located\n",
    "    \n",
    "    >output_path\n",
    "    new destination directory for the non-square images\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for filename in os.listdir(input_path):\n",
    "        image_path = os.path.join(input_path, filename)\n",
    "        if os.path.isfile(input_path):\n",
    "            with Image.open(input_path) as im:\n",
    "                width, height = im.size\n",
    "                if width != height:\n",
    "                    new_filepath = os.path.join(output_path, filename)\n",
    "                    os.rename(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a1675-a87d-42ed-844f-75f09eebcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/sra/temp'\n",
    "output_path='/Users/sra/temp3'\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    print(filename)\n",
    "    input_path = os.path.join(inputPath, file_name)\n",
    "            output_path = os.path.join(outputPath, \n",
    "                                       file_name.replace(oldExtension,\n",
    "                                                         newExtension))\n",
    "\n",
    "    try:\n",
    "        img=Image.open(filename)\n",
    "        img_size=img.size\n",
    "        if img_size[0]!=img_size[1]:\n",
    "            print('not the same size')\n",
    "        else:\n",
    "            print(\"it's the same size\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd63ad4-4c69-4512-9c63-a0b047950b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "filename = os.path.join('/Users/sra/temp/patch_nofire_857.jpg')\n",
    "img = Image.open(filename)\n",
    "img_size=img.size\n",
    "if img_size[0]!=img_size[1]:\n",
    "    print('not the same size')\n",
    "else:\n",
    "    print(\"it's the same size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914fc57-5779-427a-a99a-c95ff715a07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beec68b-76fb-4b17-bc54-dd5d7fe4f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputPath_nofire_city='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/patch_nofire_city'\n",
    "# inputPath_nofire_farm='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/patch_nofire_farm'\n",
    "# inputPath_fire_fire1='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/patch_fire_fire1'\n",
    "# inputPath_fire_fire2='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/patch_fire_fire2'\n",
    "\n",
    "# inputPaths=[inputPath_nofire_city,\n",
    "#             inputPath_nofire_farm,\n",
    "#             inputPath_fire_fire1,\n",
    "#             inputPath_fire_fire2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968d977-fbc5-49a9-bd8d-bd240a7af582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inp in (inputPaths):\n",
    "#     fileDeleter(source=inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5b861-0b8f-4de5-807a-a08385364d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetNumberedFilenames(source):\n",
    "    '''\n",
    "    Resets the numbers within a filename so that \n",
    "    they start from zero and increase \n",
    "    by 1 for all the files in a directory.\n",
    "    \n",
    "    ----\n",
    "    Inputs:\n",
    "    \n",
    "    >source\n",
    "    the directory where the files are located\n",
    "    \n",
    "    ----\n",
    "    Outputs:\n",
    "    \n",
    "    >N/A\n",
    "    renames files in-place, no further output\n",
    "    '''\n",
    "    \n",
    "    # Sort the files in the directory by name\n",
    "    files = sorted(os.listdir(source))\n",
    "    print('1',len(files))\n",
    "    \n",
    "    # Use a regular expression to search for the number in each filename\n",
    "    pattern = re.compile(r'\\d+')\n",
    "    print('2',pattern)\n",
    "    \n",
    "    # Loop over each file from the source directory\n",
    "    for i, filename in enumerate(files):\n",
    "        \n",
    "        # Get the file extension\n",
    "        file_extension = os.path.splitext(filename)[1]\n",
    "        print('3',file_extension)\n",
    "        \n",
    "        # Use the regular expression to find the number in the filename\n",
    "        number_match = pattern.search(filename)\n",
    "        print('4',number_match)\n",
    "        \n",
    "        if number_match:\n",
    "            number = int(number_match.group())\n",
    "            print('5',number)\n",
    "            \n",
    "        else:\n",
    "            number = i\n",
    "            print('6,else',number)\n",
    "        \n",
    "        # Create the new filename with the desired format\n",
    "        new_filename = f'{os.path.splitext(filename)[0].split(\"_\")[0]}_{i}{file_extension}'\n",
    "        print('7',new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(source, filename), os.path.join(source, new_filename))\n",
    "        print('8, renamed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35c53e-0228-48ee-bc15-7257746e535a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf3157d-142e-4d0a-8d0a-60aa32e0e99c",
   "metadata": {},
   "source": [
    "## Clip raster to polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edd61e-5440-44d6-947c-c532b39c20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "\n",
    "# Define the input raster and polygon mask\n",
    "input_raster = \"/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/m_3411849_se_11_060_20180722.tif\"\n",
    "mask = \"/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/perimeters_sm/santa_monica_fire_perimeters_fire_valid.shp\"\n",
    "\n",
    "# Open the input raster and polygon mask\n",
    "raster_ds = gdal.Open(input_raster)\n",
    "mask_ds = ogr.Open(mask)\n",
    "\n",
    "# Get the mask layer\n",
    "mask_lyr = mask_ds.GetLayer()\n",
    "\n",
    "# Get the extent of the mask layer\n",
    "mask_extent = mask_lyr.GetExtent()\n",
    "\n",
    "# Set the output file name and format\n",
    "output_file = \"path/to/clipped_raster.tif\"\n",
    "output_format = \"GTiff\"\n",
    "\n",
    "# Set the output file resolution\n",
    "output_res = raster_ds.GetGeoTransform()[1]\n",
    "\n",
    "# Define the output file size\n",
    "output_width = int((mask_extent[1] - mask_extent[0]) / output_res)\n",
    "output_height = int((mask_extent[3] - mask_extent[2]) / output_res)\n",
    "\n",
    "# Define the warp options\n",
    "warp_options = gdal.WarpOptions(cutlineDSName=mask, cropToCutline=True, dstSRS=raster_ds.GetProjection(), outputBounds=mask_extent, xRes=output_res, yRes=output_res, width=output_width, height=output_height)\n",
    "\n",
    "# Call the gdal.Warp() function to clip the raster\n",
    "clipped_raster_ds = gdal.Warp(output_file, raster_ds, options=warp_options)\n",
    "\n",
    "# Save clipped raster to a shapefile\n",
    "output_shp = \"path/to/clipped_raster.shp\"\n",
    "gdal.VectorTranslate(output_shp, clipped_raster_ds, format=\"ESRI Shapefile\")\n",
    "\n",
    "# Save clipped raster to a GeoJSON\n",
    "output_geojson = \"path/to/clipped_raster.geojson\"\n",
    "gdal.VectorTranslate(output_geojson, clipped_raster_ds, format=\"GeoJSON\")\n",
    "\n",
    "# Clean up\n",
    "raster_ds = None\n",
    "mask_ds = None\n",
    "clipped_raster_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f1d34-8cfa-412a-a4fe-80b317bc9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "\n",
    "# Define the input raster and polygon mask\n",
    "input_raster = \"/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/m_3411849_se_11_060_20180722.tif\"\n",
    "mask = \"/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/shapefiles/perimeters_sm/santa_monica_fire_perimeters_fire_valid.shp\"\n",
    "\n",
    "# Open the input raster and polygon mask\n",
    "raster_ds = gdal.Open(input_raster)\n",
    "mask_ds = ogr.Open(mask)\n",
    "\n",
    "# Get the mask layer\n",
    "mask_lyr = mask_ds.GetLayer()\n",
    "\n",
    "# Get the extent of the mask layer\n",
    "mask_extent = mask_lyr.GetExtent()\n",
    "\n",
    "# Set the output file name and format for GeoTIFF\n",
    "output_file_tif = \"path/to/clipped_raster.tif\"\n",
    "output_format_tif = \"GTiff\"\n",
    "\n",
    "# Set the output file name and format for GeoJSON\n",
    "output_file_geojson = \"path/to/clipped_raster.geojson\"\n",
    "output_format_geojson = \"GeoJSON\"\n",
    "\n",
    "# Set the output file resolution\n",
    "output_res = raster_ds.GetGeoTransform()[1]\n",
    "\n",
    "# Define the output file size\n",
    "output_width = int((mask_extent[1] - mask_extent[0]) / output_res)\n",
    "output_height = int((mask_extent[3] - mask_extent[2]) / output_res)\n",
    "\n",
    "# Define the warp options\n",
    "warp_options = gdal.WarpOptions(cutlineDSName=mask, cropToCutline=True, dstSRS=raster_ds.GetProjection(), outputBounds=mask_extent, xRes=output_res, yRes=output_res, width=output_width, height=output_height)\n",
    "\n",
    "# Call the gdal.Warp() function to clip the raster\n",
    "clipped_raster_ds = gdal.Warp('', raster_ds, options=warp_options)\n",
    "\n",
    "# Save clipped raster to GeoTIFF\n",
    "output_tif = \"path/to/clipped_raster.tif\"\n",
    "gdal.Translate(output_tif, clipped_raster_ds, format=output_format_tif)\n",
    "\n",
    "# Save clipped raster to GeoJSON\n",
    "output_geojson = \"path/to/clipped_raster.geojson\"\n",
    "gdal.Translate(output_file_geojson, clipped_raster_ds, format=output_format_geojson)\n",
    "\n",
    "# Save clipped raster to a shapefile\n",
    "output_shp = \"path/to/clipped_raster.shp\"\n",
    "gdal.VectorTranslate(output_shp, clipped_raster_ds, format=\"ESRI Shapefile\")\n",
    "\n",
    "# Clean up\n",
    "raster_ds = None\n",
    "mask_ds = None\n",
    "clipped_raster_ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a6f3-c1e0-4608-a7ca-a04a9349d714",
   "metadata": {},
   "source": [
    "## [Extrating Patches from Large Images ~~and Masks~~ for Semantic Segmentation](https://www.youtube.com/watch?v=7IL7LKSLb9I)\n",
    "\n",
    "Following this tutorial to convert my large fire/nofire images into patches for neural network analysis. The code block below is from this video, with some alterations to adapt it to my use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73aca2-ee68-4ff3-89bd-1733e4ab00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "import tifffile as tiff\n",
    "\n",
    "# large_image_stack_fire=tiff.imread('/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/2018/ortho_2018_sm_fire.tif')\n",
    "\n",
    "large_image_stack_patch_fire=tiff.imread('/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/img_patch_fire2.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895f6c4-8d3c-43c0-8099-f6e0f489532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated \n",
    "# https://stackoverflow.com/questions/68224588/problem-when-using-patchify-library-to-create-patches\n",
    "\n",
    "import cv2\n",
    "\n",
    "# filepaths\n",
    "target_tiff_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/img_patch_fire2.tif'\n",
    "output_location_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches'\n",
    "\n",
    "# read large_image_stack_test\n",
    "img = cv2.imread(target_tiff_fire)\n",
    "\n",
    "# cv2.imshow('image',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd83f01-cb78-401f-9d9b-8acf1f26b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_img = patchify(img, (128,128,3), step=128)\n",
    "\n",
    "for i in range(patches_img.shape[0]):\n",
    "    for j in range(patches_img.shape[1]):\n",
    "        single_patch_img = patches_img[i, j, 0, :, :, :]\n",
    "        if not cv2.imwrite(output_location_fire + 'image_' + '_'+ str(i)+str(j)+'.jpg', single_patch_img):\n",
    "            raise Exception(\"Could not write the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e6cfe-06c3-4c82-af88-8fbcb0129c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b6051-744c-48cc-a717-27509692de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # city\n",
    "# directory_='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/_patch_jpg/city'\n",
    "# prefix_='patch_nofire_city_'\n",
    "# start_=0\n",
    "\n",
    "# destination_folder='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/_patch_jpg/city/flow_control_resetFileNumbers0to19835'\n",
    "# destination='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/_patch_jpg/city'\n",
    "\n",
    "# if not os.path.exists(destination_folder):\n",
    "#     for dr,prf in zip(directory_,prefix_):\n",
    "#         resetFileNumbers(directory=dr,prefix=prf,start=test_start)\n",
    "        \n",
    "#     # create the full path to the new folder\n",
    "#     new_folder_path = os.path.join(destination, 'flow_control_resetFileNumbers0to19835')\n",
    "\n",
    "#     # create the new folder if it doesn't already exist\n",
    "#     if not os.path.exists(new_folder_path):\n",
    "#         os.makedirs(new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee48dd-f0a8-4063-a471-02f584316fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97f2e8f-942e-4435-921f-5b45a4814e3d",
   "metadata": {},
   "source": [
    "#### Rename files and change to proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3912a-d14f-44d7-ba86-db4233bebbc5",
   "metadata": {},
   "source": [
    "Convert filenames from `patch_fire.X.tif` to `patch_fire_X.tif`, where X is a number with one or more digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5e5e79a-6684-4ed0-881f-df5efb6785a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileRenamer(source:str, prefix:str,extension='.tif'):\n",
    "    '''\n",
    "    Renames files to the format provided by the user.\n",
    "    It can help clean an image format to one that can be\n",
    "    read by modules like Tensorflow.\n",
    "    \n",
    "    ----\n",
    "    Inputs:\n",
    "    \n",
    "    >source\n",
    "    the directory where the files are located\n",
    "    \n",
    "    >prefix\n",
    "    the base part of the filename that will remain\n",
    "    \n",
    "    >extension\n",
    "    defaults to '.tif', but this will ensure you only rename\n",
    "    certain files that have the specified extension\n",
    "    \n",
    "    ----\n",
    "    Outputs:\n",
    "    \n",
    "    >N/A\n",
    "    renames files in-place, no further output\n",
    "    '''\n",
    "\n",
    "    # loop over each file from the source directory\n",
    "    for filename in os.listdir(source):\n",
    "        \n",
    "        # check if the file is the provided `ext` (extension)\n",
    "        if filename.endswith(extension):\n",
    "            \n",
    "            # split the filename into base and extension\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            \n",
    "            # split the base into the prefix and number parts\n",
    "            prefix, number = base.split('.', 1)\n",
    "            \n",
    "            # create the new filename with the desired format\n",
    "            new_filename = f'{prefix}_{number}{ext}'\n",
    "            \n",
    "            # rename the file\n",
    "            os.rename(os.path.join(source, filename), \n",
    "                      os.path.join(source, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "442b8e64-2d9d-4d2d-af31-cc9b8c4ea81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a for loop to rename all the files\n",
    "\n",
    "source_train_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/fire'\n",
    "source_train_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/nofire'\n",
    "source_valid_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/fire'\n",
    "source_valid_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/nofire'\n",
    "source_test_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/fire'\n",
    "source_test_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/nofire'\n",
    "\n",
    "\n",
    "sources=[source_train_fire,\n",
    "        source_train_nofire,\n",
    "        source_valid_fire,\n",
    "        source_valid_nofire,\n",
    "        source_test_fire,\n",
    "        source_test_nofire]\n",
    " \n",
    "# flow control\n",
    "if filePresenceSumChecker(directory=source_train_fire,extension='.tif')<0:\n",
    "    for src in sources:    \n",
    "        fileRenamer(source=src,prefix='patch_fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "efcd8445-34a2-440e-a58f-299170573e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = '/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/nofire'\n",
    "extension = '.jpg'\n",
    "\n",
    "filePresenceSumChecker(directory=directory,extension=extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bcca3b54-4760-478e-bba1-2d910ff14652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for loop\n",
    "\n",
    "inputPath_train_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/fire'\n",
    "inputPath_train_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/nofire'\n",
    "inputPath_valid_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/fire'\n",
    "inputPath_valid_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/nofire'\n",
    "inputPath_test_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/fire'\n",
    "inputPath_test_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/nofire'\n",
    "\n",
    "\n",
    "outputPath_train_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/fire'\n",
    "outputPath_train_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/nofire'\n",
    "outputPath_valid_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/fire'\n",
    "outputPath_valid_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/nofire'\n",
    "outputPath_test_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/fire'\n",
    "outputPath_test_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/nofire'\n",
    "\n",
    "\n",
    "inputPaths=[inputPath_train_fire,\n",
    "            inputPath_train_nofire,\n",
    "            inputPath_valid_fire,\n",
    "            inputPath_valid_nofire,\n",
    "            inputPath_test_fire,\n",
    "            inputPath_test_nofire]\n",
    "\n",
    "outputPaths=[outputPath_train_fire,\n",
    "            outputPath_train_nofire,\n",
    "            outputPath_valid_fire,\n",
    "            outputPath_valid_nofire,\n",
    "            outputPath_test_fire,\n",
    "            outputPath_test_nofire]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9b22205a-723f-4c9e-a134-5d6294896be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow control\n",
    "if filePresenceSumChecker(directory=outputPath_train_fire,extension='.jpg')==0:\n",
    "    for inp,outp in zip(inputPaths,outputPaths):\n",
    "        imageConverter(inputPath=inp,outputPath=outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "12b672d7-7c49-4ba6-bb28-98cef133c49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1164"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filePresenceSumChecker(directory='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/fire',\n",
    "                      extension='.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c5411-30bb-4260-b73b-e2d48cd2ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2bbb2d9-ba8d-4c3b-a507-871152722ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/validation/nofire'\n",
    "\n",
    "input_path='/Users/sra/temp'\n",
    "output_path='/Users/sra/temp3'\n",
    "\n",
    "if filePresenceSumChecker(directory=output_path,extension='.jpg') != 0:\n",
    "    separateNonSquareImages(input_path=input_path,output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5787a77-1f33-4ae3-ab6e-dbab16761ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for loop\n",
    "\n",
    "inputPath_train_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/fire'\n",
    "inputPath_train_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/train/nofire'\n",
    "inputPath_valid_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/fire'\n",
    "inputPath_valid_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/validation/nofire'\n",
    "inputPath_test_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/fire'\n",
    "inputPath_test_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/test/nofire'\n",
    "\n",
    "destPath_train_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/train/fire'\n",
    "destPath_train_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/train/nofire'\n",
    "destPath_valid_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/validation/fire'\n",
    "destPath_valid_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/validation/nofire'\n",
    "destPath_test_fire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/test/fire'\n",
    "destPath_test_nofire='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/test/nofire'\n",
    "\n",
    "inputPaths=[inputPath_train_fire,\n",
    "            inputPath_train_nofire,\n",
    "            inputPath_valid_fire,\n",
    "            inputPath_valid_nofire,\n",
    "            inputPath_test_fire,\n",
    "            inputPath_test_nofire]\n",
    "\n",
    "destPaths=[destPath_train_fire,\n",
    "          destPath_train_nofire,\n",
    "          destPath_valid_fire,\n",
    "          destPath_valid_nofire,\n",
    "          destPath_test_fire,\n",
    "          destPath_test_nofire]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39e02f3e-2957-4b34-b20a-40c9ede65e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_location='/Users/sra/Desktop/Data_Science_2023/_capstone/00_capstone_data/orthoimagery/patches/nonsquares/validation/nofire'\n",
    "\n",
    "if filePresenceSumChecker(directory=checked_location,extension='.jpg') == 0:\n",
    "    for inp,des in zip(inputPaths,destPaths):\n",
    "        moveNonSquareImages(source_folder=inp,destination_folder=des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50c442-720d-41a8-a326-d677816d39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def createTVTS_edit(start, total_img: int, step=1,\n",
    "              valid_frac=0.15, test_frac=0.15,\n",
    "              replace=False, verbose=True, debug=False):\n",
    "    '''\n",
    "    Creates three lists for a train/validation/test split of \n",
    "    numbered files, such as patches previously made from \n",
    "    a larger image to be used in convolutional neural network \n",
    "    workflows.\n",
    "    \n",
    "    The training fraction of the output is the remainder of\n",
    "    the sum of the validation fraction and the testing fraction:\n",
    "    \n",
    "    train_frac = 1 - (valid_frac + test_frac)\n",
    "    \n",
    "    Default splits are:\n",
    "        0.7    = 1 - (   .15     +    .15 )\n",
    "    \n",
    "    Please ensure that you have a reasonable split amongst these\n",
    "    three groups.\n",
    "    \n",
    "    ----\n",
    "    Inputs:\n",
    "    \n",
    "    > start: starting number for the image patches\n",
    "    > total_img: serves both as total size of images in the patch set\n",
    "    > step: defaults to 1, the step size in creating a list of numbers\n",
    "    > valid_frac: the fraction of the numbers that will be split into the\n",
    "        validation set. Please make the number between 0 and 1\n",
    "    > test_frac: the fraction of the numbers that will be split into the\n",
    "        test set. Please make the number between 0 and 1\n",
    "    > replace: since this function is splitting the numbers, replace defaults\n",
    "        to False\n",
    "    > verbose: runs a line of code to check that the splitting was successful\n",
    "    > debug: helpful print statements to show you what step function is on.\n",
    "        defaults to not showing these statements\n",
    "    \n",
    "    ----\n",
    "    Outputs:\n",
    "    \n",
    "    > train_valid_test_tuple: a tuple of three lists, containing the train, valid, and\n",
    "        test list that when combined together are the same size as the total_img value\n",
    "    '''\n",
    "    \n",
    "    # create list with each image's number\n",
    "    # there are `total_img` images each in the fire and nofire datasets\n",
    "    file_nums = np.arange(start, start + total_img, step)\n",
    "    if debug:\n",
    "        print(f'created initial list of size {total_img}')\n",
    "        print(file_nums)\n",
    "        \n",
    "    # create train fraction\n",
    "    train_frac = 1 - (valid_frac + test_frac)\n",
    "    if debug:\n",
    "        print(f'created train_fraction ({train_frac})')\n",
    "    \n",
    "    # create train, valid, and test splits    \n",
    "    trains = np.random.choice(file_nums, size=int(total_img * train_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created train list')\n",
    "        print(trains)\n",
    "    \n",
    "    valids = np.random.choice(np.setdiff1d(file_nums, trains), size=int(total_img * valid_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created validation list')\n",
    "    \n",
    "    tests = np.random.choice(np.setdiff1d(file_nums, np.concatenate((trains, valids))),\n",
    "                             size=int(total_img * test_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created test list')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'The size of train ({len(trains)}), validation ({len(valids)}), and tests ({len(tests)}) together is {len(trains)+len(valids)+len(tests)}')\n",
    "        if debug:\n",
    "            print('printed size of train, validation, and test')\n",
    "    \n",
    "    return train_valid_test_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0de885-e5c6-4d34-8f54-1d110eb45b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nofire\n",
    "# run function\n",
    "# total_sum is defined in a cell above\n",
    "train_valid_test_tuple=createTVTS(start=9919,total_img=19836,\\\n",
    "                                  step=1,verbose=True,\\\n",
    "                                  debug=True)\n",
    "# train_valid_test_tuple\n",
    "\n",
    "# sanity checks\n",
    "nofire_trains=train_valid_test_tuple[0]\n",
    "nofire_valids=train_valid_test_tuple[1]\n",
    "nofire_tests=train_valid_test_tuple[2]\n",
    "\n",
    "print(len(nofire_trains))\n",
    "print(len(nofire_valids))\n",
    "print(len(nofire_tests))\n",
    "\n",
    "print(set(nofire_trains) & set(nofire_valids) & set(nofire_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae886f3f-e81b-46bf-8457-cc8931d1701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def createTVTS_edit(start, total_img: int, step=1,\n",
    "              valid_frac=0.15, test_frac=0.15,\n",
    "              replace=False, verbose=True, debug=False):\n",
    "    '''\n",
    "    Creates three lists for a train/validation/test split of \n",
    "    numbered files, such as patches previously made from \n",
    "    a larger image to be used in convolutional neural network \n",
    "    workflows.\n",
    "    \n",
    "    The training fraction of the output is the remainder of\n",
    "    the sum of the validation fraction and the testing fraction:\n",
    "    \n",
    "    train_frac = 1 - (valid_frac + test_frac)\n",
    "    \n",
    "    Default splits are:\n",
    "        0.7    = 1 - (   .15     +    .15 )\n",
    "    \n",
    "    Please ensure that you have a reasonable split amongst these\n",
    "    three groups.\n",
    "    \n",
    "    ----\n",
    "    Inputs:\n",
    "    \n",
    "    > start: starting number for the image patches\n",
    "    > total_img: serves both as total size of images in the patch set\n",
    "    > step: defaults to 1, the step size in creating a list of numbers\n",
    "    > valid_frac: the fraction of the numbers that will be split into the\n",
    "        validation set. Please make the number between 0 and 1\n",
    "    > test_frac: the fraction of the numbers that will be split into the\n",
    "        test set. Please make the number between 0 and 1\n",
    "    > replace: since this function is splitting the numbers, replace defaults\n",
    "        to False\n",
    "    > verbose: runs a line of code to check that the splitting was successful\n",
    "    > debug: helpful print statements to show you what step function is on.\n",
    "        defaults to not showing these statements\n",
    "    \n",
    "    ----\n",
    "    Outputs:\n",
    "    \n",
    "    > train_valid_test_tuple: a tuple of three lists, containing the train, valid, and\n",
    "        test list that when combined together are the same size as the total_img value\n",
    "    '''\n",
    "    \n",
    "    # create list with each image's number\n",
    "    # there are `total_img` images each in the fire and nofire datasets\n",
    "    file_nums = np.arange(start, start + total_img, step)\n",
    "    if debug:\n",
    "        print(f'created initial list of size {total_img}')\n",
    "        print(file_nums)\n",
    "        \n",
    "    # create train fraction\n",
    "    train_frac = 1 - (valid_frac + test_frac)\n",
    "    if debug:\n",
    "        print(f'created train_fraction ({train_frac})')\n",
    "    \n",
    "    # create train, valid, and test splits    \n",
    "    trains = np.random.choice(file_nums, size=int(total_img * train_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created train list')\n",
    "        print(trains)\n",
    "    \n",
    "    valids = np.random.choice(np.setdiff1d(file_nums, trains), size=int(total_img * valid_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created validation list')\n",
    "    \n",
    "    tests = np.random.choice(np.setdiff1d(file_nums, np.concatenate((trains, valids))),\n",
    "                             size=int(total_img * test_frac), replace=False)\n",
    "    if debug:\n",
    "        print('created test list')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'The size of train ({len(trains)}), validation ({len(valids)}), and tests ({len(tests)}) together is {len(trains)+len(valids)+len(tests)}')\n",
    "        if debug:\n",
    "            print('printed size of train, validation, and test')\n",
    "    train_valid_test_tuple=tuple(trains,valids,tests)\n",
    "    return train_valid_test_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b969ee-c7fc-4a91-b9ae-e9de26d4b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nofire\n",
    "# run function\n",
    "# total_sum is defined in a cell above\n",
    "train_valid_test_tuple=createTVTS_edit(start=9919,total_img=19836,\\\n",
    "                                  step=1,verbose=True,\\\n",
    "                                  debug=True)\n",
    "# train_valid_test_tuple\n",
    "\n",
    "# sanity checks\n",
    "nofire_trains=train_valid_test_tuple[0]\n",
    "nofire_valids=train_valid_test_tuple[1]\n",
    "nofire_tests=train_valid_test_tuple[2]\n",
    "\n",
    "print(len(nofire_trains))\n",
    "print(len(nofire_valids))\n",
    "print(len(nofire_tests))\n",
    "\n",
    "print(set(nofire_trains) & set(nofire_valids) & set(nofire_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08697f18-f925-4db0-8aa8-5afb98b80a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_by_index(source_dir, destination_dir, numbers_list):\n",
    "    # Get a list of all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    # Initialize a counter to keep track of the file index\n",
    "    index = 0\n",
    "    # Loop through the files and copy the ones with the specified indices\n",
    "    for i in range(len(files)):\n",
    "        # Check if the current file is in the list of indices\n",
    "        if index in numbers_list:\n",
    "            # Construct the source and destination paths for the file\n",
    "            source_path = os.path.join(source_dir, files[i])\n",
    "            destination_path = os.path.join(destination_dir, files[i])\n",
    "            # Copy the file to the destination directory\n",
    "            shutil.copy(source_path, destination_path)\n",
    "        # Increment the index counter\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f108d92-c4a6-4ca7-b938-28708768f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_by_index(source_dir, dest_dir, index_list):\n",
    "    file_list = os.listdir(source_dir)\n",
    "    for index in index_list:\n",
    "        if index < len(file_list):\n",
    "            filename = file_list[index]\n",
    "            src_path = os.path.join(source_dir, filename)\n",
    "            dst_path = os.path.join(dest_dir, filename)\n",
    "            shutil.copyfile(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"Index {index} is out of range for the source directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479bf245-f693-4d36-8ec3-8ccf75d9371b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
